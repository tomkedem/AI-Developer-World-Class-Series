---
title: "פרק 2 - ממוצע, חציון וסטיית תקן בלי סיבוכים"
weight: 3
---

# פרק 2 - ממוצע, חציון וסטיית תקן -- בלי סיבוכים
## מה זה "מרכז" של דאטה
כשאנחנו אוספים נתונים, הדבר הראשון שאנחנו מנסים להבין הוא איפה "מרוכז"
רוב המידע.
אנשים עושים את זה אינטואיטיבית כל הזמן:
כששואלים אותך כמה זמן לוקח להגיע לעבודה, אתה לא מחשב
נוסחה.
אתה זורק מספר שקרוב לתחושת הבטן שלך.
זו תפיסה של **מרכז**.

במתמטיקה, המרכז הזה נקרא "מדד מרכזי", והוא עוזר לנו להבין את הדאטה בלי
להסתכל על כל ערך בנפרד.

יש שני מדדים מרכזיים שבדרך כלל מספיקים לרוב עבודת
ה-AI:

-   **ממוצע**

-   **חציון**

שניהם מתארים את אותה שאלה: "מה הערך שמסכם את הדאטה בצורה הטובה
ביותר?"
אבל כל אחד מהם עושה את זה בדרך אחרת לגמרי.

**ממוצע -- הקול של כולם**

הממוצע מספר לנו מה הערך שמתקבל אם "נפזר את העומס" שווה בשווה בין כל
הנקודות.
הוא נותן משקל לכולם -- כולל לחריגים.

לכן אם רוב האנשים מרוויחים 10,000 ש\"ח וחמישה מנהלים מרוויחים חצי
מיליון
הממוצע "נמשך" למעלה, למרות שהוא לא מייצג אף אדם אמיתי.

הממוצע טוב כשאין ערכים חריגים.
הוא פחות טוב כשיש פיזור קיצוני.

**חציון -- הקול של האמצע**

החציון לא מבצע חישובים מסובכים.
הוא פשוט שואל:
**"מה הערך שנמצא בדיוק באמצע הרשימה?"**

זו דרך מדויקת ויציבה יותר להבין מה **רוב** האנשים מרגישים.
היא לא מושפעת ממיעוט קיצוני.

לכן בשכר, בזמן תגובה של שרתים, במספר ביקורים באתר או בכל דאטה עם "זנב
ארוך"
חציון הוא כלי הרבה יותר שימושי מממוצע.


**למה זה חשוב למפתחי AI?**

כי מודלים לומדים מתוך המספרים שאנחנו מזינים להם.שמתחפש לדפוס
אם הנתונים **מוטים**, אם הם **מעוותים**, ואם
יש בהם **ערכים חריגים** המודל ילמד דפוס שגוי.
הוא לא יודע להתעלם מרעש, הוא פשוט לומד את מה שהוא רואה.

לכן השאלה "מהו המרכז?" היא לא שאלה אקדמית.
זו שאלה פרקטית לגמרי שיכולה לקדם מודל קדימה -- או להפיל
אותו.

ברגע שמבינים ממוצע וחציון ברמה אינטואיטיבית, הדאטה מפסיק להיות "רשימת
מספרים", ומתחיל להפוך **לתמונה אמיתית** שמראה מה באמת קורה
שם.


## מה זה "פיזור" ולמה זה משנה
אחרי שמבינים איפה מרכז הדאטה נמצא, מגיעה השאלה השנייה:
**עד כמה הנתונים מפוזרים סביב המרכז הזה?**

שני datasets יכולים לקבל אותו ממוצע ואותו חציון, ועדיין
להיות שונים לגמרי.

לדוגמה:

-   קבוצה אחת שבה כל הערכים כמעט זהים

-   קבוצה שנייה שבה חלק מהערכים נמוכים מאוד וחלק קופצים
    לשמים

על הנייר זה נראה "אותו דבר".
במציאות -- אלו שני עולמות שונים לגמרי.
ופה נכנס רעיון הפיזור.

**פיזור -- כמה הדאטה "רועש"**

פיזור הוא מושג פשוט:
**כמה רחוקות הנקודות מהמרכז?**
אם רוב הנקודות צמודות למרכז -- זה דאטה יציב וצפוי.
אם הן מתפזרות לכל הכיוונים -- יש רעש שמקשה על מודל ללמוד דפוס
ברור.


דמיין שתי קבוצות של זמני תגובה (Latency) בשרת:

-   קבוצה אחת: 88, 90, 92, 91, 89

-   קבוצה שנייה: 20, 30, 400, 450, 15

יש להן חציון דומה, אבל הפיזור שונה לחלוטין.
במערכת אמיתית -- הקבוצה השנייה תהיה סיוט תפעולי.

**למה פיזור חשוב ל-AI?**

כי מודל לומד מהממוצע והחציון -- אבל חי בתוך הפיזור.
פיזור גדול מדי אומר:

-   המודל יתקשה להבחין בדפוסים

-   התוצאות יהיו לא יציבות

-   אותם נתונים יובילו לפעמים לתחזיות שונות

-   המודל "יתבלבל" ויזוז לכיוונים אקראיים בזמן האימון

פיזור נמוך אומר שהדאטה נקי, עקבי ואמין יותר.
ברוב פרויקטי ה-AI, לא המרכז מנבא הצלחה או
כישלון
**אלא רמת הפיזור.**

מתכנתי AI שמסתכלים על המרכז ולא על הפיזור
בד"כ מפספסים את התמונה הגדולה:
האם יש כאן דפוס אמיתי -- או רק רעש שמתחפש לדפוס?

**סטיית תקן -- המדד הפשוט לפיזור**

כאן מגיע כלי שנשמע מסובך אבל הוא הכי פשוט בסיפור:
**סטיית תקן**.

זו דרך למדוד במדויק כמה רחוקות הנקודות מהממוצע.
היא לוקחת את כל הפיזור, "ממירה" אותו למספר אחד,
ומאפשרת להשוות datasets שונים בצורה ברורה.

סטיית תקן גבוהה = דאטה רועש.
סטיית תקן נמוכה = דאטה יציב.

ברוב המערכות -- בעיקר בתחזיות, NLP, מודלים התנהגותיים
ובקרת איכות
סטיית תקן היא אחד המדדים הכי שימושיים שיש.


## דוגמאות על נתונים אמיתיים
כדי שהמושגים "ממוצע", "חציון" ו"סטיית תקן" לא יישארו באוויר, צריך לראות
אותם על נתונים אמיתיים.
ברגע שרואים מספרים מהעולם האמיתי -- הכול מתחיל להרגיש הרבה יותר
ברור.

ניקח שלושה סוגים קלאסיים של דאטה שמופיעים כמעט בכל פרויקט
AI:

1.  **זמני תגובה במערכת (Latency)**

נניח שאתה בודק זמני תגובה של שרת לאורך היום:
98, 102, 100, 101, 99, 103

הממוצע והחציון כמעט זהים.
הסטייה קטנה.
כך בדיוק נראית מערכת יציבה.

עכשיו תסתכל על סדרה אחרת מרובת עומסים:
40, 120, 300, 25, 500, 30

כאן סטיית התקן עצומה.
מבחינת מודל, זו מערכת שמציגה **התנהגות כאוטית**.
אי אפשר למצוא דפוס ברור.


2.  **מחירי מוצרים**

נניח שאתה מנתח מחירים של מוצר מסוים ברשת:

-   רוב החנויות: 230--260 ₪

-   חנות אחת בטעות מוכרת ב-5,000 ₪

-   חנות אחרת מוכרת בסייל ב-99 ₪

מה הממוצע יגיד לך?
משהו באמצע בין כולם -- אבל **לא מייצג אף נקודה אמיתית**.

החציון, לעומת זאת, מתעלם מהחריגות ומחזיר תמונה נקייה יותר.
ככה מתכנתי AI נמנעים מהטיות שנגרמות מערכים
קיצוניים.

3.  **דירוגי משתמשים**

במערכות המלצה (Recommenders),

משתמשים נותנים ציון 1--5.
אם מוצר מקבל:
5, 4, 5, 4, 1

הממוצע ייפול לאזור ה-3.8.
אבל הציון 1 עשוי להיות פשוט משתמש מתוסכל ביום רע.

פה סטיית התקן עוזרת להבין את העומק:

-   סטייה נמוכה ← רוב הדירוגים עקביים

-   סטייה גבוהה ← משהו לא יציב, אולי התנהגות
    חשודה

מודלים שמבינים את הפיזור יכולים לזהות אנומליות הרבה יותר
טוב.

**למה הדוגמאות האלו חשובות?**

כי בעולם האמיתי, דאטה אף פעם לא "נקי".
תמיד יש:

-   רעש

-   טעויות הזנה

-   ערכים קיצוניים

-   פערים בין ערכים

-   תקופות שקטות מול תקופות עמוסות

כשמפתח מבין את המרכז והפיזור של הדאטה, הוא יודע:

-   איך להכין את הדאטה למודל

-   איפה לנקות

-   איפה להחליק

-   ואיפה להיזהר שלא להרוס מידע חשוב

סטטיסטיקה בסיסית היא לא "קישוט".
היא חלק מהאיכות של המודל.


## הדגמה קצרה ב-NumPy
אחרי שהבנו את הרעיונות מאחורי ממוצע, חציון וסטיית תקן דרך דוגמאות
מהעולם האמיתי, הגיע הזמן לראות איך זה נראה בקוד.
וזה בדיוק המקום שבו NumPy עושה את החיים פשוטים בצורה כמעט
מגוחכת.

NumPy נועדה לעבוד עם מספרים במהירות וביעילות.
בכל מה שקשור לדאטה -- היא הכלי שהופך מתמטיקה "על נייר" למשהו שמתחבר
ישירות למערכות אמיתיות.

**נתחיל מדאטה קטן ופשוט**

ניקח רשימה שמייצגת לדוגמה זמני תגובה במערכת:
```py
import numpy as np

latencies = np.array([98, 102, 100, 101, 99, 103])
```
זה כל מה שצריך כדי להתחיל לעבוד.

**ממוצע ← נקודת האמצע של כל הערכים**
```py
avg = np.mean(latencies)
print("Average:", avg)
```
NumPy עושה את החישוב בשורה אחת, בלי לולאות ובלי קוד
"מרובע".

**חציון ← מה שקורה באמצע באמת**
```py
median = np.median(latencies)
print("Median:", median)
```

כאן רואים שהחציון נשאר רגוע גם אם יש ערכים קיצוניים.

**סטיית תקן ← האם הדאטה רגוע או רועש**
```py
std = np.std(latencies)
print("Standard deviation:", std)
```
זה המספר שמספר למודל אם יש כאן תבנית יציבה או כאוס.

**לראות הכול ביחד**
```py
print(f"Data center average: {avg}, median: {median}")
print(f"System noise standard deviation: {std}")
```

**מה רואים מהתוצאות?**

-   אם הממוצע והחציון קרובים **←** הדאטה מאוזן

-   אם סטיית התקן נמוכה **←** המערכת יציבה

-   אם סטיית התקן גבוהה **←** יש התנהגות רועשת שהמודל
    יתקשה ללמוד ממנה

זוהי אנליזה בסיסית, אבל כזו שכל מפתח AI משתמש בה כמעט בכל
פרויקט --- גם כשלא שמים לב.

**למה חשוב לראות את זה בקוד?**

כי ברגע שמפעילים את זה על נתונים אמיתיים בפרויקט שלך, המספרים מפסיקים
להיות "מושגים מתמטיים", והופכים לכלים אמיתיים שעוזרים:

-   לנקות דאטה

-   לזהות בעיות

-   להבין דפוסים

-   לשפר מודלים

והכי חשוב!

זה נותן תחושת שליטה.
פתאום סטיית תקן כבר לא נשמעת כמו הגדרה מספרית, אלא כמו כלי עבודה
ברור.

