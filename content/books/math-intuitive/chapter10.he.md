---
title: "פרק 10 - Gradient Descent [] - הלמידה עצמה"
weight: 11
---
# פרק 10 -- Gradient Descent [] --הלמידה עצמה

## רעיון ירידת המפל

בפרקים הקודמים דיברנו על עקומת הטעות, על מינימום, ועל
שיפוע.
עכשיו אנחנו מחברים את כל החלקים יחד לתהליך שהופך מודל "לא
מאומן"
למשהו שמסוגל להבין דפוסים: **Gradient
Descent**.

זה השלב שבו המודל מפסיק "לבהות" בדאטה
ומתחיל ממש **ללמוד**.

**ירידת מפל -- המנגנון שמוריד את הטעות**

Gradient Descent הוא פשוט רעיון מתמטי שמדמה ירידה על
מדרון.
אם אתה עומד על גבעה ורוצה להגיע לבסיס שלה,
הדרך הטבעית ביותר היא לזוז בכל פעם **לקראת הכיוון שבו הקרקע
יורדת**.

אותו עיקרון בדיוק קורה במודל:
הוא \"מחפש\" את הירידה בעקומת הטעות,
ומתקדם צעד אחר צעד לנקודה עם טעות נמוכה יותר.

**למה זה נקרא "מפל"?**

כי השיפוע מייצג את התלילות של השטח.
כשערך השיפוע גדול, המפל חד יותר.
כשערך השיפוע קטן, כבר כמעט הגענו למשטח שטוח ליד המינימום.

במילים פשוטות:
**המודל גולש לאורך העקומה עד שהוא מתקרב לנקודה שנוחה לו.**

**אינטואיציה קצרה**

אם אתה נמצא במקום גבוה על העקומה
(טעות גדולה)
השיפוע גדול
והמודל מבצע צעד גדול.

אם אתה קרוב יותר לאזור טוב
השיפוע קטן
והמודל מבצע צעד עדין יותר.

זו התאמה דינמית שמאפשרת למודל להתכנס בצורה יציבה.

**למה זה הגיוני כל כך?**

כי במקום לנסות "לנחש" איפה המינימום נמצא,
המודל משתמש במידע המקומי שהוא כבר יודע:
הוא פשוט בודק לאן השיפוע מצביע
וזז בכיוון שמוריד את הטעות.

זו ההבחנה הקריטית:
Gradient Descent לא מחפש פתרון מושלם.
הוא שואף לדפוס טוב יותר בכל צעד קטן.

וזה מה שהופך אותו לאחד הרעיונות החשובים ביותר בעולם
ה-AI.

## למה המודל זז "נגד" השיפוע

אחד המשפטים שמבלבלים מפתחים בתחילת הדרך הוא:
**המודל זז נגד השיפוע.**

למה נגד?
למה לא עם השיפוע?
הרי באופן אינטואיטיבי, אם השיפוע מצביע לכיוון מסוים -- לא כדאי ללכת
אליו?

כאן מגיעה הנקודה המהותית.

**השיפוע מצביע לכיוון שבו הטעות גדלה**

כששיפוע חיובי, זה אומר שאם נזוז מעט קדימה בכיוון הזה --
הטעות תגדל.

כששיפוע שלילי, זה אומר שאם נזוז בכיוון הזה -- הטעות
תרד.

וזה הופך את המנגנון לפשוט מאוד:
**המודל תמיד זז לכיוון שמקטין טעות.**

וכדי להקטין טעות, המודל צריך לזוז לכיוון שבו הטעות יורדת, ולא לכיוון
שבו היא עולה.

במילים אחרות:

-   השיפוע מצביע על הכיוון שבו הטעות גדלה

-   Gradient Descent זז לכיוון ההפוך, שבו הטעות קטנה

**דוגמה עם אינטואיציה של מדרון**

-   אם אתה עומד על מדרון שמטפס למעלה -- השיפוע מצביע כלפי
    מעלה

-   כדי לרדת למטה -- אתה צריך ללכת בדיוק בכיוון ההפוך

זה כל הסיפור.
זה אפילו לא מתמטיקה, זו תחושת שטח.

**מה המודל עושה בפועל?**

בכל צעד הוא מחשב את השיפוע באותה נקודה.
אם השיפוע חיובי, הוא זז שמאלה.
אם השיפוע שלילי, הוא זז ימינה.
אם השיפוע קטן מאוד, הוא מאט ומבצע שינוי קטן ועדין.

זו הסיבה ש Gradient Descent הוא תהליך יציב:
הוא לא עושה קפיצות מיותרות,
והוא תמיד מחפש כיוון שבו הטעות קטנה יותר.

**למה חשוב להבין את הרעיון הזה כמפתח?**

כי זה מייצר את ההיגיון שמאחורי כל שינוי שהמודל עושה.
הוא לא \"מנחש\" ולא \"קופץ\".
הוא פשוט בודק את השיפוע
ומתקדם בכיוון שמקטין את הטעות.

זה הסוד של כל למידת מכונה מודרנית.

בחלק הבא נראה איך קצב הלמידה משפיע על התנועה הזו,
ומה קורה כשצעד הלמידה גדול מדי או קטן מדי.


## מה קורה כשקצב הלמידה גדול מדי

כל צעד שGradient Descent עושה מושפע מפרמטר אחד חשוב
במיוחד:
**קצב הלמידה** (Learning Rate).

זה המספר שקובע כמה גדול יהיה כל צעד שהמודל מבצע כשהוא זז נגד
השיפוע.
אפשר לחשוב עליו בתור "כמה חזק אני מסובב את ההגה בכל פעם".

הבעיה היא שקצב הלמידה צריך להיות מאוזן.
אם הוא גדול מדי או קטן מדי -- האימון יוצא משליטה.

**קצב למידה גדול מדי -- המודל "קופץ" מעל המינימום**

כשקצב הלמידה גבוה מדי, קורה משהו לא נעים:
המודל מבצע צעד כל כך גדול שהוא בכלל **עובר את האזור הנמוך** ומתרסק בצד
השני של העקומה.

במקום להתקרב למינימום,
הוא קופץ מעליו
ומתגלגל מצד לצד
בלי להתייצב.

זה מוביל לבעיה קלאסית שנקראת \"אוסילציות\":
המודל פשוט לא מצליח להתייצב במקום טוב.

**למה זה קורה?**

כי השיפוע רק אומר האם לעלות או האם לרדת.
הוא לא אומר כמה.
אם קצב הלמידה גדול מדי,
הכיוון נכון
אבל העוצמה מוגזמת.

זה כמו לנסות לעצור על מדרגה
כשאתה יורד בריצה מהירה מדי -- אתה פשוט עף
קדימה.

**קצב למידה קטן מדי -- התקדמות איטית או תקיעה**

גם הצד השני בעייתי.
אם הצעדים קטנים מדי,
המודל מתקדם לאט בצורה מתסכלת
ואפילו עלול "להיתקע" באזור שבו השיפוע חלש.

הוא לא מזנק אבל גם לא מצליח לעבור את המדרון.

**איזון נכון**

הקסם קורה כשהקצב מאוזן:

-   מהיר מספיק כדי לרדת בעקומה

-   לא מהיר מדי כדי לא לקפוץ מעל המינימום

-   לא איטי מדי כדי לא לבזבז אלפי צעדים

זו האמנות שמסתתרת מאחורי אימון מוצלח.

בחלק הבא נראה הדגמה קצרה בקוד שממחישה איך זה נראה כשהקצב נכון, גדול מדי
או קטן מדי.

## הדגמת קוד שמראה ירידה למינימום

כדי לראות את כל הרעיונות שלמדנו פועלים ביחד, נבנה הדגמה קטנה
של Gradient Descent על עקומה פשוטה.
אין כאן מודל אמיתי, אלא רק פונקציית טעות אחת, אבל העיקרון זהה לחלוטין
למה שקורה באימון מודלים גדולים.

נשתמש בפונקציית טעות מהסוג שכבר ראינו:

def error(x):

    return (x - 3)\*\*2 + 2

הפונקציה הזו מגיעה למינימום כשה-x שווה 3.

עכשיו נכתוב Gradient Descent בסיסי:

import numpy as np

def error(x):

    return (x - 3)\*\*2 + 2

def error_slope(x):

    \# השיפוע של הפונקציה

    return 2\*(x - 3)

x = -1        # נקודת התחלה רחוקה מהמינימום

learning_rate = 0.1

print(\"צעדים:\")

for step in range(10):

    slope = error_slope(x)

    x = x - learning_rate \* slope

    print(step + 1, \"=\> x =\", round(x, 4), \" error =\",
round(error(x), 4))

**מה רואים כשמריצים את זה?**

1.  **הערך של x מתקרב בהדרגה ל-3**, המינימום
    של הפונקציה.

2.  בכל צעד, השיפוע מכוון את המודל לכיוון הנכון.

3.  כשתתקרב לנקודת המינימום, הצעדים נהיים קטנים ועדינים.

4.  ההתקדמות לא בקפיצות אלא בצורה יציבה ורציפה.

זה כמעט בדיוק מה שקורה באימון של מודל אמיתי.
ההבדל היחיד הוא שבמודלים גדולים לא עובדים על מספר אחד אלא על מיליוני
פרמטרים בבת אחת.

**למה הדוגמה הזו כל כך חשובה?**

כי היא מורידה את כל הרעש מסביב
ומראה בצורה הכי נקייה שאפשר את העיקרון שבאמת מניע את
הלמידה:

**המודל מחשב שיפוע
זז נגדו
וחוזר על זה שוב ושוב
עד שהוא מתקרב למקום שבו הטעות הכי נמוכה.**

אין קסם.
אין קפיצות.
רק רצף של תיקונים קטנים.

**זה כל הסיפור של Gradient Descent**

וזה אחד הרעיונות הכי חשובים שכל מפתח בעולם ה-AI צריך
להבין.

