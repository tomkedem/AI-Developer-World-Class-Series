---
title: "פרק 8 - פונקציות - איך מודל חושב"
weight: 9
---
# פרק 8 -- פונקציות -- איך מודל חושב

## קלט ← פלט בצורה הכי פשוטה

כמעט כל מודל בעולם ה-AI, מהקטנים ביותר ועד הגדולים ביותר,
פועל על רעיון אחד פשוט מאוד:
**מקבלים קלט, מבצעים עליו חישוב, ומחזירים פלט.**

זה הכול.

מודלים יכולים להיות מורכבים, עמוקים ורב שכבתיים, אבל הבסיס נשאר זהה
לחלוטין:
פונקציה.
משהו שמקבל משהו אחד ומחזיר משהו אחר.

**מה זה אומר בפועל?**

כשמודל מקבל וקטור קלט, הוא מבצע עליו סדרת חישובים.
החישובים האלה יכולים להיות פשוטים או מסובכים, אבל העיקרון נשאר
זהה:
**המודל ממפה את הקלט לפלט.**

למשל

-   וקטור של משפט ← סנטימנט חיובי או שלילי

-   תמונה ← תג "כלב", "חתול" או "אדם"

-   נתוני משתמש ← סיכוי לקניה

מאחורי כל זה לא עומדת "הבנה" של העולם, אלא **מיפוי מתמטי**.

**למה חשוב לראות מודל כפונקציה?**

כי זה מפשט את כל התמונה.
אין קסם, אין אינטואיציות נסתרות.
יש דרך אחת שבה המודל פועל:

**קלט נכנס ← המודל מעבד אותו ← פלט
יוצא.**

הפונקציה עצמה היא אוסף של חוקים שהמודל למד מהדאטה.
וזה בדיוק מה שמבדיל בין מודל "טוב" למודל "חלש":
איזו פונקציה הוא למד.

**ומה שמעניין יותר**

המודל לא רק מנסה להחזיר פלט.
הוא מנסה להחזיר **פלט נכון**.
כזה שמקטין את הטעות שלו מהעולם האמיתי.

וזה מוביל אותנו לנושא הבא:
כמעט כל הפונקציות שמודלים לומדים אפשר לצייר כעקומה.
ומודלים תמיד רוצים להגיע לחלק הנמוך של העקומה.

בחלק הבא נבין מה זו "עקומה" ולמה מודלים עושים כל מה שהם יכולים כדי לרדת
בה.

## מה זה "עקומה" ולמה כל מודל מנסה לרדת בה

כשאומרים שמודל \"לומד\", בפועל קורה דבר הרבה יותר פשוט:
הוא מנסה למצוא **מקום נמוך** על עקומה מתמטית שמייצגת את הטעות
שלו.

ואז מגיע רגע שבו הכול מתחבר:
**המודל לא מחפש תשובה. הוא מחפש מינימום.**

**מה זו בכלל "עקומה"?**

עקומה היא דרך לייצג את הקשר בין קלט לבין טעות.
לכל קלט שהמודל בוחר, הוא מייצר פלט.
הפלט הזה מושווה למציאות, וההפרש ביניהם הוא הטעות.

כשמציירים את הטעות הזו על גרף, מתקבלת עקומה.
נקודה נמוכה על העקומה אומרת \" טעות קטנה\".
נקודה גבוהה אומרת \"טעות גדולה\".

גם אם זה לא נראה כמו גרף שאתה רואה בבית ספר,
זו אותה אינטואיציה:
**יש מקום גבוה, ויש מקום נמוך.**

המודל רוצה לרדת למקום הנמוך.

**למה בכלל יש "עקומה"?**

בגלל שהמודל מנסה להתאים את עצמו לדאטה,
וכל החלטה שהוא מקבל (משקלים, פרמטרים, כיוונים במרחב)
משפיעה על כמה הוא יטעה.

אם הוא מכוון לא נכון
הטעות שלו גבוהה
והוא "נמצא" גבוה על העקומה.
אם הוא מכוון נכון יותר
הטעות יורדת
והוא יורד למקום נמוך יותר.

**למה חשוב להבין את זה כמפתח?**

כי בלי ההבנה הזו
למידת מכונה נראית כמו קסם שחור.
אבל עם ההבנה הזו
כל התהליך נהיה מאוד הגיוני:

-   יש עקומה שמודדת טעות

-   אנחנו רוצים להיות כמה שיותר נמוך

-   כל צעד של המודל הוא ניסיון להתקרב לנקודה נמוכה יותר

זו כל תורת האימון ב-ML במשפט אחד.

**
**

**עקומה לא חייבת להיות דו-ממדית**

כשאתה חושב על "עקומה", אל תחשוב על גרף חלק שמצויר על לוח.
במודלים אמיתיים
העקומה נמצאת במרחב של אלפי או מיליוני משתנים.
אבל האינטואיציה נשארת בדיוק אותה אינטואיציה:
יש מקומות גבוהים ויש מקומות נמוכים.

ולכן כל המודלים בעולם מגיעים לאותה מטרה:
**להקטין טעות.**
וכדי להקטין טעות, הם חייבים לרדת על העקומה.

בחלק הבא נבין למה "מינימום" הוא כל כך חשוב,
ומה בעצם קורה כשהמודל סוף סוף מגיע לשם.



## מה זה בכלל מינימום

אחרי שהבנו שהמודל נע על עקומה של טעות ושואף לרדת כל הזמן,
מגיע הרעיון שמחזיק את כל תהליך הלמידה: **המינימום**.

זו המילה שכל מפתח שומע שוב ושוב באימון מודלים.
אבל מה זה בעצם אומר?

**מינימום הוא המקום שבו המודל טועה הכי מעט**

כשמציירים את הטעות כעקומה,
יש נקודה אחת (או כמה נקודות) שבה הטעות היא הכי נמוכה.

זו נקודת המינימום.

שם המודל מרגיש "נוח":
הוא מפיק פלטים שמתאימים לדאטה טוב יותר מאשר בכל נקודה אחרת על
העקומה.

במילים פשוטות:
**מינימום = המקום שבו המודל עובד הכי טוב על הדאטה שראה.**

**אבל לא כל מינימום הוא אותו דבר**

יש שני סוגים חשובים:

1.  **מינימום מקומי**
    אזור נמוך, אבל לא הנמוך ביותר.
    אפשר לדמיין גבעה קטנה בתוך עמק גדול יותר.
    המודל עלול "להיתקע" שם, וזה חלק טבעי מהאימון.

2.  **מינימום גלובלי**
    הנקודה הכי נמוכה בכל העקומה.
    זה המקום שבו המודל מגיע לביצועים הכי טובים שהוא מסוגל.

במודלים גדולים, מרחב העקומות מורכב מאוד,
ולכן ההבחנה הזו חשובה -- גם אם אנחנו לא מציירים את העקומה
בפועל.

**למה המודל לא תמיד מגיע למינימום הטוב ביותר?**

כי העקומה שבה הוא נע היא מאוד מחוספסת.
יש הרבה ירידות ועליות, הרבה \"בורות\" קטנים,
והמודל פשוט נע לפי הכיוון שמקטין טעות ברגע מסוים.

הוא לא רואה את כל המרחב,
הוא רואה רק את השיפוע המקומי.

וזה מספיק טוב ברוב המקרים.

**למה מפתחים צריכים להבין מינימום?**

כי זה מסביר הרבה התנהגויות של מודלים:

-   לפעמים האימון \"נתקע\"

-   לפעמים המודל לא משתפר מעבר לנקודה מסוימת

-   לפעמים שינוי קטן בדאטה מזיז את כל המינימום

-   לפעמים הירידה איטית מדי או מהירה מדי

-   ולפעמים יש כמה פתרונות טובים, לא רק אחד

ברגע שמבינים שמודל רק מחפש נקודה נמוכה על עקומה
ולא \"מנסה להבין\" משהו עמוק יותר
הכול נהיה ברור.

זה הרגע שבו לומדים להבין את המודל כיצירה מתמטית,
ולא כקופסה שחורה.

**
**
