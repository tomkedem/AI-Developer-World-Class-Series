---
title: "פרק 7 - זווית ודמיון קוסינוס"
weight: 8
---
# פרק 7 -- זווית ודמיון קוסינוס

## דמיון ככיוון ולא כמרחק

בפרק הקודם ראינו שמרחק בין וקטורים הוא כלי שימושי, אבל יש לו מגבלות
ברורות.
בעיקר בעולם של טקסט, שבו המשמעות לא תמיד יושבת על \"עד כמה שני משפטים
רחוקים אחד מהשני\", אלא על השאלה **לאיזה כיוון הם מצביעים**.

כאן נכנס הרעיון החשוב הזה:
**דמיון אמיתי בין שני וקטורים נקבע לפי הכיוון שלהם, לא רק לפי המרחק
הגיאומטרי ביניהם.**

**למה הכיוון חשוב יותר מהמרחק?**

תחשוב על שני משפטים שונים מבחינת המילים, אבל דומים מאוד
ברעיון:
\"אני מאחר לעבודה\"
ו
\"אני מתקשה להגיע בזמן\"

המשמעות שלהם זורמת באותו כיוון.
גם אם הוקטורים רחוקים יחסית, זווית קטנה ביניהם מספרת שהם \"פונים\"
לאותו כיוון.

לעומת זאת:
\"אני אוהב קפה\"
ו
\"אני אוהב לשרוף גשרים\"

מרחק פשוט יגיד ששני המשפטים קרובים, כי הם חולקים מילים
דומות.
אבל הכיוון שלהם שונה לגמרי.
הם לא מתארים את אותו רעיון.

זו בדיוק הסיבה שמודלים מודרניים מסתמכים הרבה יותר על **זווית** ופחות על
מרחק.

**מה בעצם מודדים בזווית?**

כששני וקטורים מצביעים לאותו כיוון
(גם אם יש ביניהם מרחק גדול)
הזווית ביניהם קטנה.

כששני וקטורים מצביעים לכיוונים שונים
הזווית גדולה.

במילים פשוטות:
**הזווית מספרת למודל האם שני אובייקטים \"נעים סביב אותו
רעיון\".**

וזה נכון לטקסט, למשתמשים, למוצרים ולתמונות.

**למה זה כל כך חשוב בעולם ה-AI?**

כי רוב המשמעות שאנחנו מזהים בעולם היא לא \"כמה זה דומה
פיזית\",
אלא
**כמה זה דומה רעיונית**.

וקטורים שמתקרבים בכיוון
גם כשהם לא קרובים במרחק
מספרים למודל:
\"זה דומה במהות\".

זה מה שמאפשר למודלים להבין ש:

-   שמחה וזכיה מתקרבות באותו כיוון

-   עצב ואובדן נעות בכיוון דומה

-   \'לקוח מאוכזב\' ו\'שירות לא תקין\' קרובים במשמעות
    גם אם המילים שונות לחלוטין.

בחלק הבא נראה איך משתמשים בזווית הזו כדי למדוד דמיון בין שני משפטים
בפועל.

## איך מודדים דמיון בין שני משפטים

עכשיו כשהרעיון של דמיון ככיוון - ברור, אפשר לעבור לשאלה
המעשית:
**איך מחשבים בפועל את הדמיון בין שני משפטים?**

המודלים המודרניים לא בודקים אם שתי מילים זהות או דומות
בצליל.
הם משווים בין שני וקטורים שמייצגים את המשמעות של כל משפט.
וכדי להשוות בין שני וקטורים, משתמשים במדד שנקרא **דמיון
קוסינוס**.

**מה זה דמיון קוסינוס?**

זה מדד שמודד
**לא כמה הוקטורים רחוקים**,
אלא
**כמה הם מצביעים לאותו כיוון**.

ערך גבוה (קרוב ל-1) אומר שהמשפטים דומים מאוד במשמעות.
ערך נמוך (קרוב ל-0 או שלילי) אומר שהמשמעות שונה.

**
**

**איך זה נראה בקוד?**

נניח שיש לנו שני וקטורים שמייצגים משפטים,
v1 ו-v2:

import numpy as np

def cosine_similarity(a, b):

    \# מכפלה סקלרית - מודדת עד כמה שני הוקטורים \"פונים\" לאותו
כיוון

    dot = np.dot(a, b)

    \# אורך של כל וקטור - מאפשר לנטרל את השפעת הגודל שלהם

    norm_a = np.linalg.norm(a)

    norm_b = np.linalg.norm(b)

    \# חלוקה של המכפלה הסקלרית באורכים - נותנת מספר בין -1
ל-1

    \# ערך קרוב ל-1 = כיוון דומה מאוד = משמעות דומה

    \# ערך קרוב ל-0 = כמעט אין קשר בכיוון

    \# ערך שלילי = כיוונים מנוגדים

    return dot / (norm_a \* norm_b)

\# שני וקטורים עם משמעות קרובה

v1 = np.array(\[0.2, 0.8, 0.4\])

v2 = np.array(\[0.25, 0.75, 0.35\])

\# הדפסה של מידת הדמיון הסמנטי ביניהם

print(cosine_similarity(v1, v2))

אם תקבל מספר גבוה, נניח 0.93,
זה אומר שהמשפטים מצביעים על אותו רעיון.

אם המספר נמוך, נניח 0.18,
המשפטים שונים במובן עמוק.

**מה עושה np.dot?**

np.dot(a, b) מחשבת את **המכפלה הסקלרית** בין שני וקטורים.

בפועל זה אומר:
היא מכפילה כל רכיב ברכיב שמתאים לו, ואז מחברת את כל
התוצאות.

לדוגמה:

a = \[1, 2, 3\]

b = \[4, 5, 6\]

np.dot(a, b)

\# 1\*4 + 2\*5 + 3\*6 = 32

המשמעות:
ככל ששני וקטורים מצביעים בכיוון דומה יותר, הערך של dot
גדול יותר.

**מה עושה np.linalg.norm?**

np.linalg.norm(a) מחשבת את **האורך** של הוקטור
a.
זה מספר אחד שמייצג כמה הוקטור "רחוק מהמרכז".

לדוגמה:

v = \[3, 4\]

np.linalg.norm(v)

\# sqrt(3\*3 + 4\*4) = 5

המשמעות:
וקטורים גדולים או קיצוניים יקבלו נורמה גדולה יותר.

**עכשיו הדוגמה של cosine similarity הופכת
לברורה**

import numpy as np

def cosine_similarity(a, b):

    dot = np.dot(a, b)         \# כמה הכיוונים שלהם דומים

    norm_a = np.linalg.norm(a) \# אורך של הוקטור הראשון

    norm_b = np.linalg.norm(b) \# אורך של הוקטור השני

    return dot / (norm_a \* norm_b)

וקטורים שמצביעים לאותו כיוון יקבלו ערך קרוב ל 1
וקטורים בכיוונים שונים יקבלו ערך נמוך.

v1 = np.array(\[0.2, 0.8, 0.4\])

v2 = np.array(\[0.25, 0.75, 0.35\])

print(cosine_similarity(v1, v2))

-   תוצאה גבוהה, למשל **0.93**, אומרת שהמשפטים דומים מאוד
    במשמעות.

-   תוצאה נמוכה, למשל **0.18**, אומרת שהם שונים לחלוטין.

**למה דמיון קוסינוס עובד כל כך טוב?**

1.  **הוא מתעלם מהמרחק האבסולוטי**
    לא מעניינת אותנו כמות המידע.
    מעניינת אותנו המשמעות.

2.  **הוא מתמקד בכיוון המשמעותי**
    הוא מוצא את \"קו המחשבה\" המשותף בין שני משפטים.

3.  **הוא רגיש להקשרים**
    אם שתי מילים מופיעות תמיד באותו הקשר,
    הן יקבלו כיוון דומה, גם אם המספרים שונים.

4.  **הוא עמיד לשינויים קטנים בדאטה**
    תוספת של מילה או שינוי ניסוח קל
    לא מרסק את הדמיון.

לכן זה המדד הנפוץ ביותר בעולם של

-   עיבוד שפה טבעית

-   חיפוש משמעותי במסמכים

-   מערכות המלצה

-   ניתוח התנהגות משתמשים

-   כל מערכת שמשווה משמעויות ולא צורות

בחלק הבא נחבר את כל זה לעולם ה-Embeddings,
כי שם הרעיון של זוויות מתחיל לקבל משמעות אמיתית בפרויקטים
מודרניים.


## חיבור ישיר לעולם ה-Embeddings

בשלב הזה כל מה שלמדנו על וקטורים, זוויות ודמיון מתחיל להתחבר לתמונה
גדולה יותר. מאחורי כל מודל מודרני יושב מנגנון אחד שמאפשר לו לעבוד עם
טקסטים, תמונות ואפילו משתמשים: **Embedding**.

Embedding **הוא לא הוקטור עצמו, אלא המנגנון שמייצר את
הוקטורים.**

זה השער שדרכו תוכן עשיר ומורכב נכנס אל המודל ויוצא בצד השני כייצוג
מספרי מסודר.

המחשב לא מבין מילים, רעיונות או הקשרים אנושיים.
מבחינתו יש רק מספרים, מרחקים וזוויות.

כדי שהמודל יוכל \"לעבוד\" על מידע, הוא חייב לקבל אותו בצורה שמתאימה
לשפה המתמטית הפנימית שלו. כאן נכנס ה- Embedding: בזמן
האימון הוא לומד מהדאטה איזה מילים ומשפטים מופיעים יחד, אילו מקבלים
משמעות דומה ואילו רחוקים לגמרי. דרך התהליך הזה הוא בונה למודל מרחב
משמעות.

Embedding מייצר לכל פריט מידע, **וקטור**

רשימה של מספרים שממקמת אותו בתוך המרחב הזה.

**במרחב המשמעות** משפטים דומים מצביעים לכיוון דומה, מילים קשורות
מופיעות זו ליד זו, ורעיונות שונים נמצאים בכיוונים מנוגדים.

זו הסיבה שכל מה שלמדנו על וקטורים, מרחקים וזוויות לא היה \"עוד פרק
מתמטי\".
זה היה הבסיס לשפה שבה Embedding מתרגם את העולם האנושי
לעולם שהמודל מסוגל לעבוד עליו באמת.

**כדי להבין את זה בצורה ברורה, נתחיל בהמחשה חזותית.**

![תמונה שמכילה טקסט, צילום מסך, גופן, מספר תוכן בינה מלאכותית גנרטיבית
עשוי להיות שגוי.](media/image2.png){width="4.454966097987752in"
height="2.0738615485564305in"}
באיור זה רואים איך המילה עוברת דרך Embedding ומקבלת
וקטור.

הוקטור הזה לא מתאר אותיות ולא מייצג דקדוק.
הוא מייצג **משמעות דרך מספרים**. 
**איך Embeddings ממקמים משמעות במרחב**

אחרי שכל משפט או מילה קיבלו וקטור, המודל ממקם אותם במרחב שבו משמעות
דומה מקבלת כיוון דומה.

לדוגמה
מילים כמו חתול, גור וחיה יופיעו באותו אזור של המרחב.
ומילים כמו דפדפן ענן וצבע יהיו רחוקות מהן לגמרי.

![](media/image3.png){width="3.90625in" height="2.057292213473316in"}

המרחקים פחות חשובים.
הכיוון הוא מה שמספר למודל מה דומה למה.

**איך כיוון קובע דמיון**

כאן נכנס לתמונה דמיון קוסינוס.
זה המדד שמודד עד כמה שני וקטורים מצביעים לאותו כיוון.

-   אם הזווית קטנה -- רעיון דומה

-   אם הזווית רחבה -- רעיון שונה
    האורך כמעט לא משנה, רק הזווית.

![](media/image4.png){width="5.629706911636045in"
height="2.5000951443569552in"}

זו הסיבה שדמיון במרחב Embeddings נקבע לפי הכיוון ולא לפי
המרחק.

**דוגמה אחת שמאגדת את הכל**

נניח שיש לנו Embedding של המשפט
**"החתול קפץ על השולחן"**

ונרצה למצוא משפטים דומים.

המודל לא יחפש מילים דומות.
הוא יחפש **כיוון דומה במרחב הוקטורים**.

לכן הוא ימצא משפט כמו:
**"הגור טיפס על הספה"**
כי הוא מצביע לאותו רעיון, גם אם אין מילים משותפות.

משפטים על טכנולוגיה או מזג אוויר יהיו רחוקים לחלוטין.
הכיוון שלהם אחר.

**למה Embeddings הם שכבת היסוד של מודלים
מודרניים**

כי הם מאפשרים לייצג כמעט כל דבר בעולם
מילים, משפטים, משתמשים, מוצרים, תמונות ומסמכים
בצורה שהמודל יודע לחשב עליה.

זה מה שמאפשר למודלים "להבין" מחשבה אנושית דרך מתמטיקה.
לכן Embeddings נמצאים בלב של:

-   מערכות חיפוש

-   מערכות המלצה

-   מודלי שפה

-   זיהוי תמונות

-   ניתוח מסמכים

-   ועוד הרבה מנגנונים שממירים עולם אמיתי לוקטורים של
    משמעות.

