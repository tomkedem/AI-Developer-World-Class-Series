---
title: "פרק 11 - פרויקט סיום - mini_math_primer"
weight: 12
---
# פרק 11 -- פרויקט סיום -- mini_math_primer

## למה הפרויקט הזה חשוב

אחרי כל הפרקים שעברת עד עכשיו, אתה כבר מחזיק את כל היסודות שצריך כדי
להבין מודלים בצורה אינטואיטיבית ומעשית.
אבל הסבר תיאורטי לא מספיק, צריך לראות את זה קורה.
והכול באמת מתיישב רק כשכותבים קוד.

זה בדיוק התפקיד של הפרויקט הזה:
**לחבר את הבנה של מתמטיקה אינטואיטיבית עם תכלס -- כמה שורות
פייתון.**

זה לא פרויקט גדול ולא מערכת מורכבת.
זה סט של תרגילים קטנים, קטנים ממש, שכל אחד מהם מדגים רעיון אחד
מהספרון.
כל תרגיל אמור להיות משהו שאתה כותב ב-2--5 דקות.
בלי עומס, בלי קפיצה למים עמוקים.

הפרויקט הזה נותן לך שלושה דברים:

1.  **יישום אמיתי של כל הרעיונות שלמדנו**
    לא רק להבין -- אלא לראות אותם עובדים.

2.  **קוד קצר שמייצר \"קליק\" בראש**
    ברגע שאתה רואה את זה מתקמפל, הטיעון המתמטי הופך למשהו
    טבעי.

3.  **תשתית מעולה להמשך הסדרה**,
    במיוחד לספר הבא על מתמטיקה יישומית, שבו נעמיק בנגזרות, מטריצות
    ותנועות מורכבות יותר.

**איך בנוי הפרויקט?**

בכל תרגיל יש:

-   הסבר קצר

-   קטע קוד בסיסי

-   משימה קטנה להרחבה

-   שורת הדפסה שמראה תוצאה ברורה

המטרה היא לא לבחון אותך. המטרה היא לייצב את
האינטואיציה.

**נתחיל מהתחלה**

בחלקים הבאים נכתוב את כל התרגילים לפי הסדר:

1.  **ממוצע וסטיית תקן**

2.  **הסתברות מותנית**

3.  **בייס על טבלה**

4.  **נורמה**

5.  **cosine similarity**

6.  **צעד Gradient Descent**

כל תרגיל יופיע כ\"חלק\" נפרד כדי שתוכל לעקוב בקלות.

## תרגיל: ממוצע וסטיית תקן

תרגיל קצר שממחיש איך אפשר להבין "מרכז" ו"פיזור" בעזרת מספר שורות
פייתון.

**מה עושים כאן?**

ניקח רשימת מספרים פשוטה, נחשב את **הממוצע**, את **סטיית
התקן**, ונראה איך שני המספרים האלו ביחד מספרים סיפור ברור
על הנתונים.

**קוד בסיסי**
```py
import numpy as np

# Example list of numbers
data = np.array([10, 12, 13, 9, 8, 15, 14])

# Mean
mean = np.mean(data)

# Standard deviation
std = np.std(data)

print("Mean:", mean)
print("Standard deviation:", std)
```
**מה צריך לראות?**

-   **הממוצע** אומר איפה "מרכז" המספרים.

-   **סטיית התקן** אומרת כמה המספרים רחוקים מהמרכז.

סטייה גדולה מצביעה על פיזור רחב, סטייה קטנה על נתונים
מרוכזים.

**משימה קצרה**

שנה את הרשימה לנתונים "קיצוניים" יותר, למשל:
```py
data = np.array([10, 10, 11, 12, 50])
```

ושים לב איך סטיית התקן "קופצת" בגלל הערך החריג.

## תרגיל: הסתברות מותנית

תרגיל זה מחזק את הרעיון של \"הסתברות בתוך הקשר\".
במקום לבדוק את **השכיחות הכוללת של אירוע מסוים, אנחנו בודקים את השכיחות
שלו בתוך קבוצה מצומצמת**.

**הדרך לפתור את זה מתחילה בבניית טבלה של אירועים.**

נבנה טבלה פשוטה של אירועים, ואז נחשב את ההסתברות המותנית:
כמה מתוך האירועים בקבוצה מסוימת מקיימים תנאי נוסף.

**קוד בסיסי**

נניח טבלה פשוטה של הודעות:
```py
import numpy as np

# Data format is [spam, not spam]
with_free = np.array([42, 3])
without_free = np.array([1158, 797])

total_spam = with_free[0] + without_free[0]
total_ham = with_free[1] + without_free[1]

# Conditional probability. How many messages contain free inside spam
p_free_given_spam = with_free[0] / total_spam

# Conditional probability. How many messages contain free inside non spam
p_free_given_ham = with_free[1] / total_ham

print("Probability to see free inside spam:", round(p_free_given_spam, 3))
print("Probability to see free inside non spam:", round(p_free_given_ham, 3))

```
**מה צריך לראות?**

-   ההסתברות ל-"free" בתוך ספאם גבוהה
    משמעותית.

-   בתוך הודעות רגילות כמעט לא מופיעה "free".

-   זה מראה איך הקשר משנה את התמונה.

זוהי ההבנה המרכזית של הסתברות מותנית.


**משימה קצרה**

שנה את המספרים בטבלה.
נסה לדמות מצב שבו רוב ההודעות הן רגילות אבל עדיין free
קשור חזק לספאם.
שים לב איך ההסתברות המותנית ממשיכה להראות את הדפוס הנכון גם כשהיחסים
הכלליים משתנים.

## תרגיל: בייס על טבלה

תרגיל זה ממשיך ישירות מהתרגיל הקודם.
נבין איך משלבים בין התמונה הכללית לבין ההקשר המקומי כדי לקבל הערכה
אמיתית של "כמה סביר שזה נכון".

זה בדיוק מה שבייס עושה, אבל בלי נוסחאות מורכבות.
רק טבלה, ספירה, וחישוב אחד קטן.

**מה נעשה כאן?**

נחשב את הסיכוי שהודעה היא ספאם בהינתן שמופיע בה
free,
באמצעות הטבלה הפשוטה.

**קוד בסיסי**

נשתמש באותה טבלת שכיחויות:
```py
import numpy as np

# Table format is [spam, not spam]
with_free = np.array([42, 3])
without_free = np.array([1158, 797])

# Totals
total_spam = with_free[0] + without_free[0]
total_ham = with_free[1] + without_free[1]
total_all = total_spam + total_ham

# Basic probability of spam
p_spam = total_spam / total_all

# Probability to see free inside each category
p_free_given_spam = with_free[0] / total_spam
p_free_given_ham = with_free[1] / total_ham

# Probability to see free in general
p_free = (with_free[0] + with_free[1]) / total_all

# Bayes rule. Probability of spam given free
p_spam_given_free = (p_free_given_spam * p_spam) / p_free

print("Probability of spam given free:", round(p_spam_given_free, 3))

```

**מה צריך לראות?**

-   זו פשוט דרך לשלב בין מה שקורה בעולם בכלל
    לבין מה שקורה בתוך ההקשר המעניין אותנו.

-   אם free מופיע בעיקר בספאם,
    בייס משקף בדיוק את זה.

-   שינוי קטן במספרים יכול להוציא תוצאות שונות, מה שמחזק את
    הרעיון: **הקשר משנה הכול.**

**משימה קצרה**

שנה את המספרים כך שיהיו הרבה יותר הודעות רגילות בכלל,
אבל free עדיין יופיע ברוב המוחלט של הודעות
הספאם.

בדוק איך זה משפיע על תוצאת בייס,
ואיך ההקשר מנצח על התמונה הכללית.



## תרגיל: נורמה

תרגיל זה מדגים בצורה הכי פשוטה מה זה "אורך" של וקטור,
ואיך מספר אחד יכול לסכם את כל המידע שמפוזר לאורך רשימה של
ערכים.

**מה עושים כאן?**

נחשב את הנורמה של כמה וקטורים שונים,
ונראה איך אורך הוקטור משתנה בהתאם לערכים שלו.

**קוד בסיסי**
```py
import numpy as np

# Simple vector
v1 = np.array([3, 4])

# Norm (length)
norm_v1 = np.linalg.norm(v1)

print("Norm of v1:", norm_v1)

# Longer vector
v2 = np.array([1, 2, 3, 4, 5])
norm_v2 = np.linalg.norm(v2)

print("Norm of v2:", round(norm_v2, 3))
```

**מה צריך לראות?**

-   [הוקטור \[3, 4\] מקבל נורמה של 5.]
    זה האורך שלו במרחב הדו-ממדי.

-   לוקטור ארוך עם ערכים יותר "חזקים" תהיה נורמה גדולה
    משמעותית.
    זה מייצג אובייקט שנמצא "רחוק יותר" מהמרכז.

-   מודלים משתמשים בנורמה כדי להבין חוזק, קיצוניות, או עד כמה האובייקט
    "רחוק" מהאזור הרגיל של הדאטה.

**משימה קצרה**

נסה ליצור שני וקטורים:
אחד עם ערכים קטנים ואחד עם ערכים קפיציים.
חשב את הנורמות שלהם והשווה ביניהם.

לדוגמה:
```py
v_small = np.array([0.1, 0.2, 0.3])
v_jump = np.array([0.1, 5.0, 0.2])

print(np.linalg.norm(v_small))
print(np.linalg.norm(v_jump))
```
שים לב איך שינוי קטן בערך אחד יכול להגדיל מאוד את האורך של
הוקטור.


## תרגיל: cosine similarity

בתרגיל זה נמדוד דמיון בין שני וקטורים לא לפי המרחק ביניהם,
אלא לפי **הזווית** ביניהם.
זה בדיוק הכלי שמודלים משתמשים בו כדי לבדוק אם שני משפטים \"מצביעים\" על
אותו כיוון רעיוני.

**מה אנחנו עושים כאן?**

נחשב את דמיון הקוסינוס בין שני וקטורים שונים,
נראה מה קורה כשהם פונים לאותו כיוון,
ומה קורה כשהם שונים לגמרי.

**דוגמת קוד**
```py
import numpy as np

def cosine_similarity(a, b):
    dot = np.dot(a, b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    return dot / (norm_a * norm_b)

v1 = np.array([1, 2, 3])
v2 = np.array([1, 2, 3])
v3 = np.array([-3, 0, 1])

print("Similarity v1 and v2:", round(cosine_similarity(v1, v2), 3))
print("Similarity v1 and v3:", round(cosine_similarity(v1, v3), 3))

```

**מה צריך לראות?**

-   בין v1 ל-v2 הדמיון כמעט 1
    כי הם מצביעים בדיוק לאותו כיוון.

-   בין v1 ל-v3 הדמיון נמוך
    כי הם מצביעים לכיוונים שונים.

-   המדד הזה לא מתייחס למרחק.
    הוא מתייחס **רק לזווית** בין הוקטורים.
    ולכן הוא כלי מצוין לניתוח משמעות בטקסט.

**משימה קצרה**

נסה ליצור שני וקטורים שונים לחלוטין במרחק,
אבל בעלי יחס בין רכיבים דומה:
```py
a = np.array([1, 2, 3])
b = np.array([10, 20, 30])
```
בדוק את הדמיון ביניהם.
תראה שהדמיון יהיה כמעט 1, כי הכיוון זהה למרות שהגודל שונה
לגמרי.


## תרגיל: צעד Gradient Descent

תרגיל זה מחבר את כל מה שלמדנו בפרקים 8--10.
הרעיון הוא לראות איך צעד אחד של Gradient Descent משנה את
ערך הפרמטר ומקטין את הטעות.

המטרה מאוד פשוטה:
**להראות איך מחשבים שיפוע, איך זזים נגדו, ואיך הטעות יורדת בעקבות
הצעד.**

**מה אנחנו עושים כאן?**

נגדיר פונקציית טעות אחת, נמצא את השיפוע בנקודה מסוימת,
ונבצע צעד אחד של Gradient Descent.

**דוגמת קוד**
```py
import numpy as np

# Simple error function
def error(x):
    return (x - 3)**2

# The slope of the error function
def slope(x):
    return 2*(x - 3)

# Starting point
x = -1

learning_rate = 0.1

print("Before the step:")
print("x =", x, "  error =", error(x))

# One step of gradient descent
x_new = x - learning_rate * slope(x)

print("After the step:")
print("x =", round(x_new, 4), "  error =", round(error(x_new), 4))

```
**מה צריך לראות?**

-   הפונקציה מוגדרת כך שהמינימום שלה הוא ב-x = 3

-   השיפוע בנקודה x = -1 הוא חיובי

-   לכן Gradient Descent יזיז את x בכיוון
    השלילי (נגד השיפוע)

-   אחרי הצעד, x מתקרב ל-3

-   ועל הדרך, גם הטעות קטנה

זה בדיוק איך שמודלים גדולים יורדים בעקומת הטעות:
רצף של צעדים קטנים שכל אחד מהם מקטין את הטעות עוד קצת.

**משימה קצרה**

שחק עם קצב הלמידה:
```py
learning_rate = 0.8
```
ותראה מה קורה.
האם x קופץ רחוק מדי?
האם הוא עובר את המינימום?
זה מדגים בצורה ברורה למה קצב למידה חייב להיות מאוזן.