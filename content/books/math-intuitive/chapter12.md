---
title: "פרק 12 - איך כל זה מתחבר ל-ML ול-NLP"
weight: 13
---
# פרק 12 -- איך כל זה מתחבר ל-ML ול-NLP

## למה כל הדברים שלמדנו חוזרים בכל מודל

עברת דרך ארוכה.
וקטורים, נורמה, מרחק, זווית, הסתברות מותנית, בייס, שיפוע, ירידת
מפל.
לכאורה זה הרבה נושאים שונים.
אבל האמת הפשוטה היא שכל הדברים האלו חוזרים בכל מודל מודרני, קטן או
גדול.

הסיבה מאוד ברורה:
**מודלים לא "מבינים" עולם. הם מבינים מתמטיקה.**

הם לא פועלים על מילים, תמונות או משתמשים.
הם פועלים על ייצוגים מתמטיים של הדברים האלה.
ומאותו רגע, כל הכלים שראינו הופכים להיות כלי עבודה קבועים.

**למה זה נכון לכל סוג של מודל?**

-   כשמודל מקבל טקסט, הוא לא מקבל אותיות. הוא מקבל **וקטור
    ****Embedding.**

-   ההשוואה בין מילים ומשפטים נעשית דרך **זווית ומרחק** בין
    וקטורים.

-   מציאת פתרון טוב נעשית על **עקומת טעות**.

-   כל תהליך הלמידה מבוסס על **שיפוע**.

-   התקדמות לקראת פתרון נעשית דרך **Gradient
    Descent**.

-   וכל עיבוד של דאטה מתחיל מהבנה של **ממוצע, פיזור
    והקשרים**.

ככל שהמודלים גדלים, היסודות האלו לא נעלמים -- להפך.
הם הופכים להרבה יותר חשובים.

**למה זה משמעותי עבורך כמפתח?**

כי אם הבנת את הספרון הזה,
אתה כבר יודע לקרוא את השפה הפנימית של המודל.

ברגע שמישהו יגיד לך:
\"יש לנו בעיה בהטיית ה-Embedding\"
או
\"המודל תקוע במינימום מקומי\"
או
\"ה-cosine similarity לא מספיק גבוה\"
זה כבר לא נשמע כמו קסם שחור.
זה פשוט המשך טבעי של החומר שכבר למדת.

אתה לא נשאר בחוץ.
אתה חלק מהשיחה.


## איפה פוגשים אותם שוב בספרון הבא

כל הרעיונות שלמדת כאן הם לא רק בסיס טוב.
הם **השלב הראשון** לקראת הספר הבא בסדרה, שעוסק במתמטיקה יישומית
למפתחי AI.

הספר הזה ייקח את כל מה שלמדנו כאן
ויקדם אותו צעד אחד קדימה, אל העולם שבו מודלים גדולים באמת
חיים.

**מה חוזר ומעמיק שם?**

-   **וקטורים ומרחבים גדולים**

> ראינו כאן וקטורים כסדרה של מספרים.
> בספרון הבא נרחיב איך וקטורים חיים בתוך מרחבים של עשרות אלפי ממדים ומה
> קורה כשמרחקים וזוויות משתנים שם.

-   **מטריצות**
    הפעם דיברנו עליהן רק ברמת אינטואיציה.
    בספרון הבא תראה איך כל שכבה במודל היא בעצם מכפלה מטריציונית ואיך
    שילוב של מטריצות יוצר את כל מה שמודלים עושים.

-   **שיפועים מורכבים**
    דיברנו כאן על שיפוע של מספר אחד.
    בספרון הבא נפגוש שיפועים שמתפזרים על מיליוני פרמטרים
    ונראה איך מודלים גדולים מחשבים אותם ביעילות.

-   **Gradient Descent אמיתי**
    בפרק הזה ראינו רק ירידה על עקומה פשוטה.
    בהמשך נראה גרסאות מתקדמות של ירידת מפל
    כגון: Momentum, Adam ואופטימיזציות שבעולם האמיתי אי
    אפשר להסתדר בלעדיהן.

-   **Embeddings עמוקים**
    עבדנו כאן עם דוגמאות פשוטות.
    בספרון הבא תראה איך Embeddings נוצרים,
    איך לומדים אותם,
    איך משנים אותם,
    ואיך משתמשים בהם במערכות NLP אמיתיות.

**למה כדאי להמשיך לשם אחרי הספרון הזה?**

כי עכשיו יש לך את **השפה האינטואיטיבית** הנכונה.
ברגע שמבינים את אבני הבניין,
המעבר למודלים מורכבים כבר לא מאיים
אלא מעניין ומאוד הגיוני.

## מה יקל עליך כשהמודלים נהיים גדולים ומורכבים

מודלים היום גדולים יותר, עמוקים יותר, ועובדים על כמויות מידע שפעם לא
דמיינו.
ואז מגיעה השאלה:
כשהכול נהיה כל כך מורכב,
איך מפתח יכול עדיין להבין מה קורה בפנים?

התשובה פשוטה:
**מבינים רק אם מכירים את היסודות.**

החדשות הטובות הן שכשתפסת את אבני הבניין של הספרון הזה,
יש דברים שמתחילים פתאום להיראות הרבה יותר פשוטים.

**וקטורים כבר לא מסתוריים**

גם אם וקטור Embedding הוא ברוחב 4096,
זה עדיין אותו רעיון של רשימת מספרים שמייצגת משמעות.
העיקרון נשאר זהה, רק הסקייל משתנה.

**זוויות ומרחקים הופכים לכלי עבודה אמיתי**

כשאתה מבין איך דמיון קוסינוס עובד,
אתה יכול לזהות למה שני משפטים מתבלבלים,
למה חיפוש טקסט מחזיר תוצאות מסוימות,
ולמה מודל "יישר קו" בין משמעויות לא קשורות.

הכול מתחיל ונגמר בזווית בין וקטורים.

**שיפוע ו-Gradient Descent כבר לא מפחידים**

אימון של מודל ענק נראה כמו קסם עד שאתה זוכר:
הוא עדיין רק מחשב שיפועים
וזז נגדם
מיליוני פעמים.
ברגע שמבינים את המנגנון הקטן,
המנגנון הגדול כבר לא מסתורי.

**הסתברות מותנית ובייס עוזרים לקרוא תוצאות**

כשמודל טועה,
הוא לא "טיפש",
הוא פשוט למד באופן שמתאים להסתברויות ולדפוסים שהוא ראה.
ברגע שאתה יודע איך בייס חושב,
אתה מבין למה טעויות מסוימות הן צפויות.

**נורמה ומרחק מסבירים מתי נתון קופץ החוצה**

כשמשהו "לא נראה הגיוני",
זה בדרך כלל משתקף באורך חריג של וקטור
או במרחק גדול מדי במרחב. []
העין שלך כבר יודעת לזהות את זה.

**מה כל זה נותן לך כמתכנת?**

בזמן שאחרים רואים מודל כקופסה שחורה,
אתה רואה אותו כאוסף של צעדים מתמטיים פשוטים.
אתה יודע לקרוא את ההתנהגות שלו,
להסביר למה הוא פעל כפי שפעל,
ולתקן כיוונים בצורה מושכלת.

הבנה של היסודות לא נועדה להפוך אותך למתמטיקאי.
היא נועדה להפוך אותך למפתח שמסוגל לעבוד לצד מודלים גדולים
בלי להיבהל מהם.