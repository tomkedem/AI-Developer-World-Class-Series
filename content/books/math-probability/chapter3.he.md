---
title: "פרק 3 - סטטיסטיקה תיאורית (Descriptive Statistics) – הבנת הדאטה"
weight: 4

---

# **פרק 3: סטטיסטיקה תיאורית - הבנת הדאטה** 


עולם ה AI מתבסס על נתונים.  
כמתכנת הפער הקריטי ביותר אינו בבחירת האלגוריתם אלא בהבנה מה באמת **מסתתר בתוך הדאטה** שאתה מאמן עליו את המודל.  

סטטיסטיקה תיאורית היא **כלי הנדסי** שמאפשר לראות את המבנה הבסיסי את הנטייה המרכזית ואת מידת הפיזור של הנתונים עוד לפני שמריצים שורת קוד אחת של אימון.

רוב הבעיות המשמעותיות ב ML כמו יציבות לא טובה Loss שקופץ או התכנסות איטית נובעות מליקויים בדאטה ולא באלגוריתם.  
היכולת לזהות **ערכים קיצוניים Outliers** התפלגות עקומה או **רעש לא צפוי** היא יכולת קריטית לשמירה על יציבות המודל.

כאן מתחילים משלושת המדדים הבסיסיים ביותר.  
שלושתם ביחד נותנים **תמונה מהירה של אופי הדאטה**.

**מדדי נטייה מרכזית ופיזור: ממוצע חציון וסטיית תקן**


## מדדי נטייה מרכזית ופיזור

| מדד | מה הוא מייצג | ההשלכה ההנדסית |
| --- | --- | --- |
| ממוצע Mean | הערך הממוצע של כלל הנתונים | מושפע מאוד מערכים קיצוניים ולכן עלול למשוך את המודל לכיוון שאינו משקף את הדאטה הטיפוסי |
| חציון Median | הערך שנמצא באמצע כך שחצי מהערכים גבוהים ממנו וחצי נמוכים ממנו | אינו מושפע מערכים קיצוניים ולכן יציב יותר כאשר הדאטה עקום או רועש |
| סטיית תקן Standard Deviation | מדד לפיזור הערכים סביב הממוצע | מצביע על יציבות הדאטה. פיזור גבוה מוביל ללמידה פחות יציבה ול Loss שקשה להתכנס |

**ממוצע לעומת חציון: רגישות לחריגות**

הממוצע רגיש מאוד לערכים קיצוניים משום שהוא משקלל כל נקודה. כאשר הדאטה רעוע או מכיל חריגות רבות החציון מציג תמונה אמינה יותר לגבי רוב הנתונים.


```py
import numpy as np

# A dataset representing response times (in ms), with one extreme outlier (250ms)
data = np.array([10, 12, 11, 9, 250])

# Calculate Mean (Affected by the outlier)
print("Mean:", np.mean(data))

# Calculate Median (Not affected by the outlier)
print("Median:", np.median(data))

# The high outlier (250) pulled the mean up significantly, while the median remains a true representation of the central tendency.
```

**סטיית תקן: מדד ליציבות**

הסטייה התקנית מספרת למודל **כמה קשה הוא יצטרך לעבוד**.  
סטייה גבוהה σ משמעה **שונות גבוהה בדאטה**.  

זהו **דגל אדום** שמרמז שהמודל יתקשה למצוא דפוס יציב וה Loss עלול להיות **קופצני ובלתי צפוי**.

```py
import numpy as np

# Data with low variance (easy for the model)
data_low_var = np.array([48, 49, 51, 52])

# Data with high variance (challenging for the model)
data_high_var = np.array([1, 10, 50, 99])

print("Low Variance Std Dev:", np.std(data_low_var))
print("High Variance Std Dev:", np.std(data_high_var))
# The model will learn much easier and faster on the low variance data.
```
מדדי הנטייה המרכזית והפיזור הם הצעד הראשון להבנת הדאטה.

## התפלגויות נפוצות והשפעתן על מודלי AI

כאשר מפתחים מסתכלים על דאטה חשוב להבין לא רק היכן נמצא **המרכז** כמו ממוצע או חציון  
והאם הדאטה **מפוזר** כמו סטיית תקן  
אלא גם **איך הערכים מתפזרים בפועל**.  

צורת ההתפלגות היא **החתימה של הדאטה** והיא קובעת כיצד המודל יכול ללמוד דפוסים יציבים.

התפלגות היא למעשה **גרף היסטוגרמה** שמראה את תדירות הופעתם של כל ערך בדאטה.

**1. התפלגות נורמלית Normal Distribution 🔔**

זוהי התפלגות הפעמון הקלאסית  
רוב הערכים מרוכזים סביב הממוצע  
והשכיחות יורדת בצורה סימטרית ככל שמתרחקים ממנו.

- **היתרון ההנדסי:**  
דאטה עם התפלגות נורמלית מספק למודל את הסביבה הלימודית **היציבה ביותר**.  
הממוצע החציון והשכיח Mode נמצאים באותו מקום והרעש בקצוות נמוך.

- **מתי נצפה לראות אותה:**  
נתונים פיזיולוגיים כמו גובה ומשקל  
מדדי שגיאה של חיישנים מכוילים היטב.

- **ההשלכה על המודל:**  
המודל לומד בצורה חלקה  
ערך ה Loss יורד באופן קבוע  
והמודל פחות רגיש לשינויים בערך ה Learning Rate.

![Normal Distribution](/AI-Developer-World-Class-Series/img/de.jpg)  

**2. התפלגות עקומה / חד-צדדית (Skewed Distribution)**

זוהי אחת ההתפלגויות הנפוצות והמטעות ביותר בדאטה אמיתי. רוב הנתונים נדחסים לצד אחד של הסקאלה, ויש "זנב" ארוך של ערכים נדירים שמושך את ההתפלגות.

- **הבעיה ההנדסית:** הזנב מושך את הממוצע הרחק מהחציון והשכיח. אם תתבסס על הממוצע, תקבל תמונה שגויה של הנתונים הטיפוסיים. לדוגמה: הכנסה ממוצעת תהיה גבוהה בהרבה מההכנסה הטיפוסית, בגלל קומץ עשירים קיצוניים.

- **ההשלכה על המודל:**

  - המודל נוטה לתת משקל יתר למקרים הנדירים בזנב.
  - פתרון נפוץ: שימוש בטרנספורמציה לוגריתמית (Log Scaling) כדי לדחוס את הזנב הארוך ולהפוך את ההתפלגות ליותר נורמלית לפני האימון.

**3. התפלגות דו-גבעית (Bi-modal Distribution) ⛰️⛰️**

זהו מצב שבו רואים שני שיאי שכיחות ברורים, מה שמצביע על שני "מרכזים" שונים בתוך אותו Dataset.

- **הבעיה ההנדסית:** המודל מקבל מסר סותר – הוא מנסה ללמוד דפוס אחד, אבל הדאטה מכיל למעשה שני דפוסים שונים בתכלית. אם המודל ינסה להתכנס למרכז משותף בין שתי הגבעות, הוא יהיה גרוע בחיזוי שני הטיפוסים.

- **מתי נצפה לראות אותה:** נתונים מעורבים, למשל: זמני שירות של לקוחות פרטיים וזמני שירות של לקוחות עסקיים (שני סוגי התנהגות שונים), או מדידות שמגיעות משני מקורות נפרדים.

- **ההשלכה על המודל:** במקום לנסות "לכפות" למידה על שתי קבוצות שונות, הדרך הנכונה היא לרוב לחלק את הדאטה לשתי תתי-קבוצות ולבנות שני מודלים נפרדים או להוסיף פיצ'ר שמזהה את הקבוצה.

**4. התפלגות רועשת / מלאת חריגות (Heavy-Tailed / Noisy) 💥**

כאן, מעבר להתפלגות הבסיסית, יש כמות גדולה באופן לא פרופורציונלי של ערכים קיצוניים (חריגים).

**הסכנה ההנדסית:** כל ערך קיצוני משפיע על ה-Loss של האימון במידה גדולה. המשקלות במודל יתוקנו בצורה דרמטית בניסיון "לרצות" את הערכים הקיצוניים, על חשבון הדיוק עבור רוב הנתונים.

---

## זיהוי חריגות ועיוותים: איך ה-Loss נפגע

לפני שהמודל בכלל מתחיל תהליך למידה, עליך לבצע בקרת איכות קפדנית על הדאטה. חריגות (Outliers) ועיוותים (Skewness) אינם רק "לכלוך" אקראי – הם כוח משיכה חזק שעלול לשנות את כל כיוון הלמידה.

**1. הדיקטטורה של החריגות: ההשפעה על הגרדיאנט**

ערך קיצוני אחד, גבוה או נמוך בצורה לא סבירה, משפיע על פונקציית ה-Loss (במיוחד MSE - Mean Squared Error) בצורה בלתי פרופורציונלית. מכיוון ש-MSE מעלה את השגיאה בריבוע, טעות גדולה מנופחת משמעותית.

- **ההשלכה על הלמידה:** במהלך Gradient Descent, המודל מנסה למצוא את הכיוון היעיל ביותר להקטנת ה-Loss. הערכים הקיצוניים יוצרים "בליטות" חדות בגרף ה-Loss, וגורמים לגרדיאנט (השיפוע/כיוון התיקון) להצביע לכיוון הרחק מהמינימום האמיתי של רוב הנתונים.

- **התוצאה:** המודל מבלה איטרציות רבות בניסיון "לרצות" את הנקודה החריגה, במקום להתמקד בדפוס היציב של רוב הדאטה. זו הסיבה המרכזית ל-Loss קופצני ולהתכנסות איטית.

**2. Z-Score: כלי מהיר לזיהוי חשודים**

כדי לאתר חריגות במהירות, ניתן להשתמש בציון תקן (Z-Score). מדד זה מנרמל את הנתונים ומספר כמה סטיות תקן (Standard Deviations) הערך הספציפי רחוק מהממוצע.

**אינטואיציה:** ה-Z-Score מחשב עד כמה הערך לא סביר ביחס לכלל הקבוצה.

- Z-Score קרוב ל-0: ערך נורמלי וטיפוסי.
- Z-Score $\geq |2|$: ערך חריג יחסית (חשוד).
- Z-Score $\geq |3|$: ערך קיצוני מאוד (לרוב דורש טיפול).

```py
import numpy as np

# Dataset of server latency (ms) with one outlier
latency_times = np.array([50, 52, 55, 60, 48, 150])

mean_val = np.mean(latency_times)
std_dev = np.std(latency_times)

# Calculate Z-scores for the entire dataset
z_scores = (latency_times - mean_val) / std_dev

print("Mean:", mean_val)
print("Std Dev:", std_dev)
print("Z-scores:", z_scores)
# The Z-score for the outlier (150) will be significantly high (e.g., > 2.5)
```

**3. פתרונות הנדסיים לחריגות**

לאחר זיהוי החריגה, המפתח צריך לקבל החלטה:

1. **ניקוי (Removal):** אם החריגה נובעת מטעות מדידה (למשל, חיישן מקולקל), יש להסיר אותה.

2. **שינוי צורה (Transformation):** אם הדאטה מעוות (Skewed), שימוש בטרנספורמציה לוגריתמית ($\log(x)$) יכול לדחוס את הזנב הארוך ולהפוך את ההתפלגות ליותר נורמלית.

3. **כיווץ (Capping):** הגבלת הערכים הקיצוניים לנקודה מסוימת (למשל, לקבוע שכל ערך מעל 99% יקבל את הערך של 99%).

פעולות אלו מבטיחות שהמודל לומד דפוסים מייצגים ויציבים, ולא רודף אחרי הרעש.

## יציבות מודל, בחירת Loss וסיכום הנדסי 

בחירת פונקציית ה-Loss (השגיאה) של המודל מושפעת ישירות מהמבנה הסטטיסטי של הדאטה. כאשר הדאטה מכיל חריגות רבות, עלינו לבחור בפונקציית Loss שהיא פחות רגישה לכוחות המשיכה של הערכים הקיצוניים.

**1. Loss Function: בחירת הכלל הנכון**

**MAE (Mean Absolute Error) vs. MSE (Mean Squared Error)**

| פונקציית Loss | הנוסחה | הרגישות לחריגות | מתי להשתמש? |
|---------------|---------|------------------|--------------|
| MSE | $\frac{1}{n}\sum (y_i - \hat{y}_i)^2$ | גבוהה מאוד | כאשר הדאטה נקי, יציב ומחולק נורמלית, ואתה רוצה "להעניש" בחומרה על טעויות גדולות. |
| MAE | $\frac{1}{n}\sum\|y_i - \hat{y}_i\|$ | נמוכה | כאשר הדאטה מכיל חריגות או עיוותים, ואתה רוצה להתעלם מהשפעתם המוגזמת. |

**האינטואיציה ההנדסית:**

**MSE** מעלה את השגיאה בריבוע, ולכן טעות בגודל 10 תורמת 100 ל-Loss. טעות בגודל 100 תורמה 10,000! ערך קיצוני אחד יכול לשלוט בגרדיאנט. **MAE**, לעומת זאת, סופר את השגיאה כפי שהיא (טעות בגודל 10 תורמת 10), ולכן הוא הרבה יותר **חסין לרעש ולחריגות**.

**המסקנה למתכנת:**

אם הניתוח הסטטיסטי הראשוני (מנה 1 ו-2) גילה התפלגות עקומה או חריגות משמעותיות, יש לשקול ברצינות להשתמש ב-**MAE** כפונקציית ה-Loss העיקרית, או לעבור לנרמול דאטה יסודי.

**2. הדגמה בקוד: סיכום Dataset קטן ואימות מבנה 📊**

כדי להבין איך סטטיסטיקה תיאורית נכנסת לעבודה היומיומית, בוא נסכם Dataset קטן. הרעיון הוא לראות איך כמה פעולות פשוטות חושפות מבנה ברור של הדאטה.

נתחיל עם אוסף ערכים שמייצגים, למשל, זמני תגובה של מערכת:


```py
import numpy as np

# Response times (in ms), with a clear outlier (300)
times = np.array([120, 130, 128, 300, 125, 127, 126])

# 1. Check Central Tendency and Dispersion
print("--- Basic Statistics ---")
print("Mean:", np.mean(times))
print("Median:", np.median(times))
print("Std Dev:", np.std(times))

# Observation: The Mean is significantly higher than the Median (148 vs 127), indicating a skewed distribution or outlier.
# The high Standard Deviation confirms the large spread.

# 2. Identify Outliers with Z-score
mean_val = np.mean(times)
std_dev = np.std(times)
z_scores = (times - mean_val) / std_dev
print("\n--- Z-scores ---")
print("Z-scores:", z_scores)
# The Z-score for 300 is > 2.5, confirming it is an extreme outlier that needs attention.
```

**ויזואליזציה של ההתפלגות (Histogram)**

לפעמים, התמונה שווה יותר מהמספרים. היסטוגרמה פשוטה יכולה לחשוף מיד התפלגות עקומה, זנבות ארוכים או קפיצות לא טבעיות.

```py
import matplotlib.pyplot as plt

plt.hist(times, bins=5, edgecolor='black')
plt.title("Distribution of Response Times")
plt.xlabel("Time (ms)")
plt.ylabel("Frequency")
plt.show()

# The visualization instantly shows a cluster of stable data and a single, isolated bar for the outlier.

```

**המסקנה ההנדסית:**

הסטטיסטיקה התיאורית גילתה מיד שקיים ערך חריג אחד (300ms) שמושך את הממוצע כלפי מעלה ומגדיל מאוד את הפיזור. לפני אימון, עלינו לטפל בערך זה כדי למנוע Loss לא יציב ולמידה מוטה.

3. **לסיכום הפרק: סטטיסטיקה כבקרת איכות למודלים**

סטטיסטיקה תיאורית היא השכבה הראשונה והחשובה ביותר של בקרת איכות במודלי AI. היא זו שקובעת:

1. **יציבות ה-Loss:** דאטה עם סטיית תקן נמוכה מייצר Loss יציב, התכנסות חלקה, ופחות תנודתיות ב-Gradient Descent.

2. **כיוון הלמידה:** ערכים קיצוניים משנים את כיוון ה-Gradient, גורמים למודל ללמוד דפוס שגוי ולהתרחק מהמינימום האמיתי.

3. **בחירת הכלים הנכונים:** מבנה הדאטה (התפלגות) מכתיב את בחירת פונקציית ה-Loss (MAE לעומת MSE), ואת הצורך בנרמול, טרנספורמציה או ניקוי.

מודלים שלא מצליחים להתכנס במקרים רבים סובלים פשוט לא מבעיית קוד, אלא מסטטיסטיקה בסיסית שלא נותחה בזמן.

**היכולת לזהות בעיות אלו בעזרת כמה שורות קוד בסיסיות היא יכולת הנדסית יקרה מפז.**